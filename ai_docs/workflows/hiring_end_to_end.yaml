# ai_docs/workflows/hiring_end_to_end.yaml
# Canonical, executable hiring workflow definition (v1.0)
# Assumes candidate JSON input and produces standardized artifacts

metadata:
  id: hiring_end_to_end
  type: workflow_spec
  domain: hiring
  created_date: 2025-08-10
  last_updated: 2025-08-10
  author: Junie
  quality_score: 9.1/10
  tags: [hiring, workflow, orchestration]
  visibility: public
  version: 1.0
  sources:
    - context\hr_processes\hiring\hiring_stages.yaml
    - artifacts\public\hiring\interview_process.md
    - ai_docs\prompts\hiring\generate_interview_kit_prompt.md
    - ai_docs\plans\candidate_screening_plan.md

inputs:
  # Primary input: normalized candidate data in JSON array
  candidates_json: "data\\public\\hiring\\resume\\candidates.json"
  # Optional: job description and role configs
  job_description: "artifacts\\public\\hiring\\job_descriptions\\backend_mid_level.md"
  role_context:
    mission_values: "context\\company_info\\mission_vision_values.yaml"
    hiring_stages: "context\\hr_processes\\hiring\\hiring_stages.yaml"
    interview_process: "artifacts\\public\\hiring\\interview_process.md"
  # Evaluator identity for GitHub collaboration on take-home submissions
  evaluator_github_handle: "<YOUR_GITHUB_HANDLE>"

principles:
  - Technical Excellence & Scalable Elegance
  - Customer-Centric Craftsmanship
  - Ownership & Proactivity
  - Observability & Guardrails
  - Data-Informed Iteration
  - Integrity & Reliability
  - Security & Compliance First
  - Collaboration & Knowledge-Sharing
  - Continuous Learning & Mentorship
  - Innovative Spirit

mcp_servers:
  - name: exa
    purpose: "Real-time search and candidate/company validation"
  - name: sequential-thinking
    purpose: "Decompose tasks and plan sub-steps deterministically"
  - name: playwright
    purpose: "Optional browser checks (portfolio, GitHub quick scan)"
  - name: fetch
    purpose: "Retrieve raw assets or JSON endpoints if needed"

quality_gates:
  - name: approval_gate_screening
    approver_role: Platform Lead
    applies_to_stages: [screening]
  - name: approval_gate_assessment
    approver_role: Platform Lead
    applies_to_stages: [assessment]
  - name: approval_gate_interview_kits
    approver_role: Platform Lead
    applies_to_stages: [interview_preparation]
  - name: content_quality_threshold
    rule: ">= 8.5/10 quality score for generated materials"

artifacts_layout:
  interview_kits: "artifacts\\public\\hiring\\interview_materials\\upcoming\\{candidate_id}\\"
  takehome_assignment: "artifacts\\public\\hiring\\takehome_assignment\\upcoming\\{candidate_id}\\"
  screening_reports: "artifacts\\public\\hiring\\evaluation\\{candidate_name}_screening_report.md"
  assignment_evaluations: "artifacts\\public\\hiring\\evaluation_sheets\\upcoming\\{candidate_id}\\takehome_evaluation.md"
  evaluation_summaries: "artifacts\\public\\hiring\\evaluation_sheets\\upcoming\\{candidate_id}\\summary.md"
  audit_log: "data\\public\\sleipnir_flow\\hiring_runs\\{run_id}.json"

schema:
  candidate:
    required: [candidate_id, full_name, contact, experience, projects, skills, education]
    optional: [core_values_evidence, red_flags, github_urls, portfolio_urls, languages]

stages:
  - id: context_engineering
    name: Context Load & Verification
    description: Load required context files and validate inputs before execution.
    actions:
      - validate_paths: ["${inputs.role_context.mission_values}", "${inputs.role_context.hiring_stages}", "${inputs.role_context.interview_process}"]
      - validate_json_schema: {file: "${inputs.candidates_json}", schema: "candidate"}
      - checklist: "data\\public\\new\\context_engineering\\project_context_implementation_checklist.md"
    outputs: []

  - id: intake
    name: Candidate Intake & Normalization
    description: Normalize candidate JSON and prepare working set for downstream tasks.
    actions:
      - normalize_profiles: {source: "${inputs.candidates_json}", ensure_fields: [candidate_id, full_name]}
      - allocate_ids: {rule: "atlas_001, nova_002 pattern if missing"}
      - redact_pii: true
    outputs:
      - working_set: "data\\public\\hiring\\working\\{run_id}\\candidates.normalized.json"

  - id: jd_mapping
    name: JD Mapping & Competency Alignment
    description: Map candidate skills and projects against JD and values.
    actions:
      - parse_jd: {file: "${inputs.job_description}"}
      - compute_skill_diff: true
      - values_alignment_scan: {values_source: "${inputs.role_context.mission_values}"}
    outputs:
      - mapping_report: "data\\public\\hiring\\working\\{run_id}\\{candidate_id}\\jd_mapping.json"

  - id: screening
    name: Automated Screening & Recommendation
    description: Apply candidate_screening_plan to produce a structured screening report and recommendation.
    references:
      - "ai_docs\\plans\\candidate_screening_plan.md"
    actions:
      - llm_analysis: {plan: "candidate_screening_plan", mode: "assistant"}
      - generate_report_md: {path: "artifacts\\public\\hiring\\evaluation\\{candidate_name}_screening_report.md"}
      - approval: {gate: approval_gate_screening}
    outputs:
      - report: "artifacts\\public\\hiring\\evaluation\\{candidate_name}_screening_report.md"

  - id: assessment
    name: Personalized Take-Home Assignment
    description: Generate tailored assignment and evaluation rubric; store under upcoming.
    actions:
      - select_template: {bank: "artifacts\\public\\hiring\\takehome_assignment\\entry_level"}
      - personalize_assignment: {inputs: [jd_mapping, screening_report]}
      - write_assignment: {path: "artifacts\\public\\hiring\\takehome_assignment\\upcoming\\{candidate_id}\\assignment.md"}
      - write_rubric: {path: "artifacts\\public\\hiring\\takehome_assignment\\upcoming\\{candidate_id}\\evaluation_sheet.md"}
      - instruct_candidate_github_collaboration: {github_handle: "${inputs.evaluator_github_handle}"}
      - approval: {gate: approval_gate_assessment}
    outputs:
      - assignment: "artifacts\\public\\hiring\\takehome_assignment\\upcoming\\{candidate_id}\\assignment.md"
      - rubric: "artifacts\\public\\hiring\\takehome_assignment\\upcoming\\{candidate_id}\\evaluation_sheet.md"

  - id: assessment_evaluation
    name: Take-Home Assignment Evaluation
    description: Evaluate the take-home submission using the standard single-assignment evaluation prompt; gate progression based on score.
    references:
      - "ai_docs\\prompts\\hiring\\evaluate_take_home_assignment_prompt.md"
    actions:
      - verify_github_collaboration: {collaborator: "${inputs.evaluator_github_handle}"}
      - gemini_run:
          prompt: "ai_docs\\prompts\\hiring\\evaluate_take_home_assignment_prompt.md"
          context: "artifacts\\public\\hiring\\takehome_assignment\\upcoming\\{candidate_id}\\assignment.md"
      - write_evaluation: {path: "artifacts\\public\\hiring\\evaluation_sheets\\upcoming\\{candidate_id}\\takehome_evaluation.md"}
      - compute_decision:
          rule: "proceed if weighted_score >= 3.0/5 (60%); else do not proceed"
    outputs:
      - evaluation: "artifacts\\public\\hiring\\evaluation_sheets\\upcoming\\{candidate_id}\\takehome_evaluation.md"
      - proceed: true

  - id: interview_preparation
    name: Interview Kit Generation
    description: Produce candidate_context.md, interview_guide.md, interview_script.md using prompt.
    references:
      - "ai_docs\\prompts\\hiring\\generate_interview_kit_prompt.md"
      - "artifacts\\public\\hiring\\interview_process.md"
    actions:
      - gemini_run:
          prompt: "ai_docs\\prompts\\hiring\\generate_interview_kit_prompt.md"
          context: "${inputs.candidates_json}"
      - write_kit:
          base_dir: "artifacts\\public\\hiring\\interview_materials\\upcoming\\{candidate_id}\\"
          files: [candidate_context.md, interview_guide.md, interview_script.md]
      - approval: {gate: approval_gate_interview_kits}
    outputs:
      - kit_dir: "artifacts\\public\\hiring\\interview_materials\\upcoming\\{candidate_id}\\"

  - id: interviewing
    name: Structured Interview Loop
    description: Conduct BEI, pair programming, system design, and deep-dive per interview_process.md.
    actions:
      - schedule_loop: {duration_hours: 3.5}
      - pair_programming_bank: "artifacts\\public\\hiring\\pair_programming\\"
      - capture_notes_templates: true
    outputs:
      - loop_schedule: "artifacts\\public\\hiring\\interview_materials\\upcoming\\{candidate_id}\\loop_schedule.md"

  - id: consolidation
    name: Post-Interview Consolidation & Scoring
    description: Aggregate rubrics and notes into a weighted summary with rationale and risks.
    actions:
      - aggregate_scores: {sources: [assessment_rubric, bei_notes, technical_notes]}
      - bias_check: true
      - generate_summary: {path: "artifacts\\public\\hiring\\evaluation_sheets\\upcoming\\{candidate_id}\\summary.md"}
    outputs:
      - summary: "artifacts\\public\\hiring\\evaluation_sheets\\upcoming\\{candidate_id}\\summary.md"

  - id: decision
    name: Decision, Offer, and Audit
    description: Final decision record, compensation banding, and audit logging.
    actions:
      - decision_record: "artifacts\\private\\hiring\\decisions\\{candidate_id}\\decision_record.yaml"
      - offer_template: "artifacts\\private\\hiring\\offers\\{candidate_id}\\offer_letter.md"
      - write_audit_log: {path: "data\\public\\sleipnir_flow\\hiring_runs\\{run_id}.json"}
    outputs:
      - decision_record: "artifacts\\private\\hiring\\decisions\\{candidate_id}\\decision_record.yaml"
      - audit_log: "data\\public\\sleipnir_flow\\hiring_runs\\{run_id}.json"

observability:
  tracing: true
  trace_id: "{run_id}"
  audit_fields: [candidate_id, stage, actor, tool, timestamp, quality_score, approver]
  export_cadence: "daily"
